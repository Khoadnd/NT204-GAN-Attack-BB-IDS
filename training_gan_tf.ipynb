{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import typing\n",
    "import numpy as np\n",
    "from const import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Critic(in_feature : int) -> tf.keras.models.Sequential:\n",
    "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "  model = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init, input_shape=(in_feature,)),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=1, kernel_initializer=init)\n",
    "    ],\n",
    "    name='Critic'\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "def Generator(latent_dim : int, out_feature : int) -> tf.keras.models.Sequential:\n",
    "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "  model = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init, input_shape=(latent_dim,)),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=out_feature, kernel_initializer=init)\n",
    "    ],\n",
    "    name='Generator'\n",
    "  )\n",
    "  \n",
    "  return model\n",
    "\n",
    "def Blackbox(path : str) -> any:\n",
    "  with open(path, 'rb') as handle:\n",
    "    return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path : str) -> pd.DataFrame:\n",
    "  X = pd.read_csv(path)\n",
    "  # Select only probe attack\n",
    "  X = X.drop(columns=['label'])[X['label'] == 1].reset_index(drop=True).astype('float32')\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_critic(real_sample):\n",
    "  with tf.GradientTape() as tape:\n",
    "    noise = tf.random.normal([len(real_sample), latent_dim])\n",
    "    fake = gen(noise, training=True)\n",
    "    fake_pred = critic(fake, training=True)\n",
    "    real_pred = critic(real_sample, training=True)\n",
    "    critic_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
    "    \n",
    "  critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "  opt_critic.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "  for var in critic.trainable_variables:\n",
    "    var.assign(tf.clip_by_value(var, clip_min, clip_max))\n",
    "  \n",
    "  return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator(batch_size):\n",
    "  with tf.GradientTape() as tape:\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    fake = gen(noise, training=True)\n",
    "    fake_pred = critic(fake, training=True)\n",
    "    blackbox_pred = tf.numpy_function(blackbox.predict, [fake], Tout=tf.float32)\n",
    "    target = tf.zeros(batch_size)\n",
    "    blackbox_loss = ids_loss(target, blackbox_pred)\n",
    "    generator_loss = tf.reduce_mean(-fake_pred + lambada * blackbox_loss)\n",
    "    \n",
    "  gen_grad = tape.gradient(generator_loss, gen.trainable_variables)\n",
    "  opt_gen.apply_gradients(zip(gen_grad, gen.trainable_variables))\n",
    "  return blackbox_loss, generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.disable_interactive_logging()\n",
    "dataset = load_dataset('dataset/train_content_feature.csv')\n",
    "latent_dim = 13\n",
    "critic = Critic(13)\n",
    "gen = Generator(latent_dim, 13)\n",
    "blackbox = tf.keras.models.load_model('models/DNN.h5')\n",
    "# blackbox = Blackbox('models/XGBoost.pickle')\n",
    "\n",
    "opt_critic = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "opt_gen = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "n_batch = 64\n",
    "n_critics = 5\n",
    "lambada = 0.5\n",
    "clip_min = -0.01\n",
    "clip_max = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Epoch: 0, Loss Critic: -4.0381564758718014e-10, Loss Gen: 0.6065590381622314, Loss Blackbox: 1.2131319046020508\n",
      "Step: 100, Epoch: 0, Loss Critic: -1.4665602066088468e-09, Loss Gen: 0.6233168840408325, Loss Blackbox: 1.2466473579406738\n",
      "Step: 200, Epoch: 0, Loss Critic: -1.3556018529925495e-09, Loss Gen: 0.6036550402641296, Loss Blackbox: 1.2073235511779785\n",
      "Step: 300, Epoch: 0, Loss Critic: -1.9931576389353722e-09, Loss Gen: 0.6231797337532043, Loss Blackbox: 1.2463723421096802\n",
      "Step: 400, Epoch: 0, Loss Critic: -1.7566890164744109e-09, Loss Gen: 0.6156391501426697, Loss Blackbox: 1.231291651725769\n",
      "Step: 500, Epoch: 0, Loss Critic: 2.7284841053187847e-12, Loss Gen: 0.6235125064849854, Loss Blackbox: 1.2470386028289795\n",
      "Step: 600, Epoch: 0, Loss Critic: -1.2973941920790821e-09, Loss Gen: 0.5703575611114502, Loss Blackbox: 1.14072847366333\n",
      "Step: 700, Epoch: 0, Loss Critic: -6.166374078020453e-10, Loss Gen: 0.6060425043106079, Loss Blackbox: 1.2120983600616455\n",
      "Step: 0, Epoch: 1, Loss Critic: -4.3019099393859506e-10, Loss Gen: 0.5985270738601685, Loss Blackbox: 1.1970674991607666\n",
      "Step: 100, Epoch: 1, Loss Critic: -1.8139871826861054e-09, Loss Gen: 0.579866886138916, Loss Blackbox: 1.1597472429275513\n",
      "Step: 200, Epoch: 1, Loss Critic: -1.4279066817834973e-09, Loss Gen: 0.571590006351471, Loss Blackbox: 1.1431933641433716\n",
      "Step: 300, Epoch: 1, Loss Critic: -2.1686901163775474e-09, Loss Gen: 0.5670089721679688, Loss Blackbox: 1.1340312957763672\n",
      "Step: 400, Epoch: 1, Loss Critic: -5.967649485683069e-09, Loss Gen: 0.5997422933578491, Loss Blackbox: 1.199497938156128\n",
      "Step: 500, Epoch: 1, Loss Critic: -3.4669938031584024e-09, Loss Gen: 0.5909875631332397, Loss Blackbox: 1.1819884777069092\n",
      "Step: 600, Epoch: 1, Loss Critic: 2.7284841053187847e-12, Loss Gen: 0.5506083369255066, Loss Blackbox: 1.1012299060821533\n",
      "Step: 700, Epoch: 1, Loss Critic: 9.094947017729282e-13, Loss Gen: 0.617249608039856, Loss Blackbox: 1.2345125675201416\n",
      "Step: 0, Epoch: 2, Loss Critic: -2.6261659513693303e-09, Loss Gen: 0.5708414316177368, Loss Blackbox: 1.1416962146759033\n",
      "Step: 100, Epoch: 2, Loss Critic: -1.9276740204077214e-09, Loss Gen: 0.6133748888969421, Loss Blackbox: 1.2267630100250244\n",
      "Step: 200, Epoch: 2, Loss Critic: -3.637978807091713e-11, Loss Gen: 0.5950934290885925, Loss Blackbox: 1.190199851989746\n",
      "Step: 300, Epoch: 2, Loss Critic: -4.547473508864641e-12, Loss Gen: 0.6010646820068359, Loss Blackbox: 1.2021424770355225\n",
      "Step: 400, Epoch: 2, Loss Critic: -1.3642420526593924e-12, Loss Gen: 0.5838630199432373, Loss Blackbox: 1.1677391529083252\n",
      "Step: 500, Epoch: 2, Loss Critic: 1.8189894035458565e-12, Loss Gen: 0.6374729871749878, Loss Blackbox: 1.2749590873718262\n",
      "Step: 600, Epoch: 2, Loss Critic: 9.094947017729282e-13, Loss Gen: 0.5799424052238464, Loss Blackbox: 1.159897804260254\n",
      "Step: 700, Epoch: 2, Loss Critic: 9.094947017729282e-13, Loss Gen: 0.5674108266830444, Loss Blackbox: 1.1348347663879395\n",
      "Step: 0, Epoch: 3, Loss Critic: 0.0, Loss Gen: 0.6187218427658081, Loss Blackbox: 1.2374566793441772\n",
      "Step: 100, Epoch: 3, Loss Critic: 0.0, Loss Gen: 0.6024432182312012, Loss Blackbox: 1.204899549484253\n",
      "Step: 200, Epoch: 3, Loss Critic: -4.547473508864641e-13, Loss Gen: 0.5771231651306152, Loss Blackbox: 1.154259443283081\n",
      "Step: 300, Epoch: 3, Loss Critic: -1.8189894035458565e-12, Loss Gen: 0.5770805478096008, Loss Blackbox: 1.1541742086410522\n",
      "Step: 400, Epoch: 3, Loss Critic: -9.094947017729282e-13, Loss Gen: 0.579494833946228, Loss Blackbox: 1.1590027809143066\n",
      "Step: 500, Epoch: 3, Loss Critic: 0.0, Loss Gen: 0.5921454429626465, Loss Blackbox: 1.184304118156433\n",
      "Step: 600, Epoch: 3, Loss Critic: -2.2737367544323206e-12, Loss Gen: 0.5987179279327393, Loss Blackbox: 1.197448968887329\n",
      "Step: 700, Epoch: 3, Loss Critic: 9.094947017729282e-13, Loss Gen: 0.5746628046035767, Loss Blackbox: 1.149338722229004\n",
      "Step: 0, Epoch: 4, Loss Critic: -2.7284841053187847e-12, Loss Gen: 0.5888587236404419, Loss Blackbox: 1.1777305603027344\n",
      "Step: 100, Epoch: 4, Loss Critic: 4.547473508864641e-13, Loss Gen: 0.5821306705474854, Loss Blackbox: 1.1642744541168213\n",
      "Step: 200, Epoch: 4, Loss Critic: -9.094947017729282e-13, Loss Gen: 0.6380056142807007, Loss Blackbox: 1.276024341583252\n",
      "Step: 300, Epoch: 4, Loss Critic: 0.0, Loss Gen: 0.5623205900192261, Loss Blackbox: 1.1246542930603027\n",
      "Step: 400, Epoch: 4, Loss Critic: 9.094947017729282e-13, Loss Gen: 0.5830449461936951, Loss Blackbox: 1.1661030054092407\n",
      "Step: 500, Epoch: 4, Loss Critic: -9.094947017729282e-13, Loss Gen: 0.6346069574356079, Loss Blackbox: 1.2692272663116455\n",
      "Step: 600, Epoch: 4, Loss Critic: -4.5929482439532876e-10, Loss Gen: 0.5484921336174011, Loss Blackbox: 1.096997618675232\n",
      "Step: 700, Epoch: 4, Loss Critic: 2.2737367544323206e-12, Loss Gen: 0.6285460591316223, Loss Blackbox: 1.2571054697036743\n"
     ]
    }
   ],
   "source": [
    "ids_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# ids_loss = log_loss\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.convert_to_tensor(dataset.values))\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(n_batch)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  for step, X_real in enumerate(train_data):\n",
    "    for _ in range(n_critics):\n",
    "      critic_loss = train_critic(X_real)\n",
    "    blackbox_loss, gen_loss = train_generator(len(X_real))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print('Step: {}, Epoch: {}, Loss Critic: {}, Loss Gen: {}, Loss Blackbox: {}'.format(\n",
    "          step, epoch, critic_loss, gen_loss, blackbox_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_accuracy(num_sample, num_sample_detected):\n",
    "  return num_sample_detected / num_sample\n",
    "\n",
    "def attack_success_rate(detection_rate_org, detection_rate_adv):\n",
    "  return detection_rate_org - detection_rate_adv\n",
    "\n",
    "def evade_increase_rate(detection_rate_org, detection_rate_adv):\n",
    "  return 1 - detection_rate_adv / detection_rate_org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = len(dataset)\n",
    "\n",
    "org_pred = blackbox.predict_proba(dataset)\n",
    "org_pred = org_pred.round().argmax(axis=1)\n",
    "num_sample_detected_org = 0\n",
    "for s in org_pred:\n",
    "  if s == 1:\n",
    "    num_sample_detected_org += 1\n",
    "\n",
    "noise = tf.random.normal([num_sample, latent_dim])\n",
    "adv_sample = gen(noise, training=False)\n",
    "adv_sample = pd.DataFrame(adv_sample.numpy(), columns=dataset.columns)\n",
    "adv_pred = blackbox.predict_proba(adv_sample)\n",
    "adv_pred = adv_pred.round().argmax(axis=1)\n",
    "num_sample_detected_adv = 0\n",
    "for s in adv_pred:\n",
    "  if s == 1:\n",
    "    num_sample_detected_adv += 1\n",
    "\n",
    "\n",
    "detection_rate_org = detection_accuracy(num_sample, num_sample_detected_org)\n",
    "detection_rate_adv = detection_accuracy(num_sample, num_sample_detected_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = attack_success_rate(detection_rate_org, detection_rate_adv)\n",
    "eir = evade_increase_rate(detection_rate_org, detection_rate_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack success rate: 0.3459620702419056, Evade increase rate: 0.3465204021547118\n"
     ]
    }
   ],
   "source": [
    "print(\"Attack success rate: {}, Evade increase rate: {}\".format(asr, eir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
