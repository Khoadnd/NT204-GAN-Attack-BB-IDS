{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import typing\n",
    "import numpy as np\n",
    "from const import *\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Critic(in_feature : int) -> tf.keras.models.Sequential:\n",
    "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "  model = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init, input_shape=(in_feature,)),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=1, kernel_initializer=init)\n",
    "    ],\n",
    "    name='Critic'\n",
    "  )\n",
    "\n",
    "  return model\n",
    "\n",
    "def Generator(latent_dim : int, out_feature : int) -> tf.keras.models.Sequential:\n",
    "  init = tf.keras.initializers.RandomNormal(stddev=0.02)\n",
    "  model = tf.keras.models.Sequential(\n",
    "    layers=[\n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init, input_shape=(latent_dim,)),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=128, kernel_initializer=init),\n",
    "      tf.keras.layers.BatchNormalization(),\n",
    "      tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "      \n",
    "      tf.keras.layers.Dense(units=out_feature, kernel_initializer=init)\n",
    "    ],\n",
    "    name='Generator'\n",
    "  )\n",
    "  \n",
    "  return model\n",
    "\n",
    "def Blackbox(path : str) -> any:\n",
    "  with open(path, 'rb') as handle:\n",
    "    return pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path : str) -> pd.DataFrame:\n",
    "  X = pd.read_csv(path)\n",
    "  # Select only probe attack\n",
    "  X = X.drop(columns=['label'])[X['label'] == 1].reset_index(drop=True).astype('float32')\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_critic(real_sample):\n",
    "  with tf.GradientTape() as tape:\n",
    "    noise = tf.random.normal([len(real_sample), latent_dim])\n",
    "    fake = gen(noise, training=True)\n",
    "    fake_pred = critic(fake, training=True)\n",
    "    real_pred = critic(real_sample, training=True)\n",
    "    critic_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
    "    \n",
    "  critic_grad = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "  opt_critic.apply_gradients(zip(critic_grad, critic.trainable_variables))\n",
    "  for var in critic.trainable_variables:\n",
    "    var.assign(tf.clip_by_value(var, clip_min, clip_max))\n",
    "  \n",
    "  return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_generator(batch_size):\n",
    "  with tf.GradientTape() as tape:\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    fake = gen(noise, training=True)\n",
    "    fake_pred = critic(fake, training=True)\n",
    "    # blackbox_pred = tf.numpy_function(blackbox.predict, [fake], Tout=tf.int64)\n",
    "    blackbox_pred = blackbox(fake, training=False)\n",
    "    # blackbox_pred = tf.one_hot(blackbox_pred, depth=5)\n",
    "    target = tf.zeros(batch_size)\n",
    "    blackbox_loss = ids_loss(target, blackbox_pred)\n",
    "    generator_loss = tf.reduce_mean(-fake_pred + lambada * blackbox_loss)\n",
    "    \n",
    "  gen_grad = tape.gradient(generator_loss, gen.trainable_variables)\n",
    "  opt_gen.apply_gradients(zip(gen_grad, gen.trainable_variables))\n",
    "  return blackbox_loss, generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.disable_interactive_logging()\n",
    "dataset = load_dataset('dataset/train_content_feature.csv')\n",
    "latent_dim = 13\n",
    "critic = Critic(13)\n",
    "gen = Generator(latent_dim, 13)\n",
    "# blackbox = Blackbox('models/ExtraTrees.pickle')\n",
    "blackbox = tf.keras.models.load_model('models/DNN.h5')\n",
    "\n",
    "opt_critic = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "opt_gen = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "n_epochs = 5\n",
    "n_batch = 64\n",
    "n_critics = 5\n",
    "lambada = 0.5\n",
    "clip_min = -0.01\n",
    "clip_max = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Epoch: 0, Loss Critic: -4.549065124592744e-09, Loss Gen: 1.574833869934082, Loss Blackbox: 3.149670124053955\n",
      "Step: 100, Epoch: 0, Loss Critic: -6.499249138869345e-09, Loss Gen: 0.0006462509045377374, Loss Blackbox: 0.0012947538634762168\n",
      "Step: 200, Epoch: 0, Loss Critic: -5.814968062622938e-09, Loss Gen: 3.3515367249492556e-05, Loss Blackbox: 6.92621833877638e-05\n",
      "Step: 300, Epoch: 0, Loss Critic: -2.8978774935239926e-09, Loss Gen: 1.509459616499953e-05, Loss Blackbox: 3.233642200939357e-05\n",
      "Step: 400, Epoch: 0, Loss Critic: 1.3642420526593924e-12, Loss Gen: 7.270203241205309e-06, Loss Blackbox: 1.6996318663586862e-05\n",
      "Step: 500, Epoch: 0, Loss Critic: 1.1687006917782128e-10, Loss Gen: 5.210602466831915e-06, Loss Blackbox: 1.2803660865756683e-05\n",
      "Step: 600, Epoch: 0, Loss Critic: -5.093511390441563e-09, Loss Gen: 4.462220204004552e-06, Loss Blackbox: 1.1136593457194977e-05\n",
      "Step: 700, Epoch: 0, Loss Critic: -5.217657417233568e-09, Loss Gen: 3.4122454053431284e-06, Loss Blackbox: 9.087742910196539e-06\n",
      "Step: 0, Epoch: 1, Loss Critic: -6.491518433904275e-11, Loss Gen: 2.352592218812788e-06, Loss Blackbox: 8.182521924027242e-06\n",
      "Step: 100, Epoch: 1, Loss Critic: -1.1368683772161603e-11, Loss Gen: 2.0360571397759486e-06, Loss Blackbox: 7.42072052162257e-06\n",
      "Step: 200, Epoch: 1, Loss Critic: -2.8421709430404007e-12, Loss Gen: 1.9322858406667365e-06, Loss Blackbox: 7.141303285607137e-06\n",
      "Step: 300, Epoch: 1, Loss Critic: 6.821210263296962e-13, Loss Gen: 1.4399057590708253e-06, Loss Blackbox: 6.169034350023139e-06\n",
      "Step: 400, Epoch: 1, Loss Critic: 4.547473508864641e-13, Loss Gen: 1.1038423508580308e-06, Loss Blackbox: 5.513398264156422e-06\n",
      "Step: 500, Epoch: 1, Loss Critic: -6.821210263296962e-13, Loss Gen: 6.539283958773012e-07, Loss Blackbox: 4.6677560021635145e-06\n",
      "Step: 600, Epoch: 1, Loss Critic: 0.0, Loss Gen: 5.383325287766638e-07, Loss Blackbox: 4.313864337746054e-06\n",
      "Step: 700, Epoch: 1, Loss Critic: 1.1368683772161603e-13, Loss Gen: 4.761617447002209e-07, Loss Blackbox: 4.187204012850998e-06\n",
      "Step: 0, Epoch: 2, Loss Critic: -2.3874235921539366e-12, Loss Gen: 3.7703566135860456e-07, Loss Blackbox: 3.999082309746882e-06\n",
      "Step: 100, Epoch: 2, Loss Critic: 1.9326762412674725e-12, Loss Gen: 2.1965007590551977e-07, Loss Blackbox: 3.697335841934546e-06\n",
      "Step: 200, Epoch: 2, Loss Critic: 5.684341886080801e-13, Loss Gen: -1.3350495464692358e-07, Loss Blackbox: 2.965323119497043e-06\n",
      "Step: 300, Epoch: 2, Loss Critic: -1.2505552149377763e-12, Loss Gen: -9.25264629358935e-08, Loss Blackbox: 3.047276322831749e-06\n",
      "Step: 400, Epoch: 2, Loss Critic: 4.547473508864641e-13, Loss Gen: -1.639300535316579e-07, Loss Blackbox: 2.903855602198746e-06\n",
      "Step: 500, Epoch: 2, Loss Critic: -1.5916157281026244e-12, Loss Gen: -3.4460617825970985e-07, Loss Blackbox: 2.542503807489993e-06\n",
      "Step: 600, Epoch: 2, Loss Critic: 1.5916157281026244e-12, Loss Gen: -1.7697038856567815e-07, Loss Blackbox: 2.877773567888653e-06\n",
      "Step: 700, Epoch: 2, Loss Critic: 2.2737367544323206e-13, Loss Gen: -3.332212088480446e-07, Loss Blackbox: 2.5723061298776884e-06\n",
      "Step: 0, Epoch: 3, Loss Critic: 1.0231815394945443e-12, Loss Gen: -5.74433386191231e-07, Loss Blackbox: 2.0898830825899495e-06\n",
      "Step: 100, Epoch: 3, Loss Critic: 1.8189894035458565e-12, Loss Gen: -5.00969235872617e-07, Loss Blackbox: 2.237029548268765e-06\n",
      "Step: 200, Epoch: 3, Loss Critic: 3.410605131648481e-13, Loss Gen: -3.450189467457676e-07, Loss Blackbox: 2.551817033236148e-06\n",
      "Step: 300, Epoch: 3, Loss Critic: -1.1368683772161603e-12, Loss Gen: -6.142323059066257e-07, Loss Blackbox: 2.017238784901565e-06\n",
      "Step: 400, Epoch: 3, Loss Critic: 2.6147972675971687e-12, Loss Gen: -7.609606882397202e-07, Loss Blackbox: 1.72666841535829e-06\n",
      "Step: 500, Epoch: 3, Loss Critic: 0.0, Loss Gen: -5.327849521563621e-07, Loss Blackbox: 2.1830135210620938e-06\n",
      "Step: 600, Epoch: 3, Loss Critic: 3.410605131648481e-13, Loss Gen: -5.618696832243586e-07, Loss Blackbox: 2.1289979486027732e-06\n",
      "Step: 700, Epoch: 3, Loss Critic: -2.7284841053187847e-12, Loss Gen: -7.953426575113554e-07, Loss Blackbox: 1.6745141238061478e-06\n",
      "Step: 0, Epoch: 4, Loss Critic: 7.958078640513122e-13, Loss Gen: -8.68838810674788e-07, Loss Blackbox: 1.5590317161695566e-06\n",
      "Step: 100, Epoch: 4, Loss Critic: 1.8189894035458565e-12, Loss Gen: -8.174536105798325e-07, Loss Blackbox: 1.6465753560623853e-06\n",
      "Step: 200, Epoch: 4, Loss Critic: 2.0463630789890885e-12, Loss Gen: -6.12049404935533e-07, Loss Blackbox: 2.020963165705325e-06\n",
      "Step: 300, Epoch: 4, Loss Critic: 3.410605131648481e-13, Loss Gen: -8.132133757499105e-07, Loss Blackbox: 1.618635678823921e-06\n",
      "Step: 400, Epoch: 4, Loss Critic: -1.7053025658242404e-12, Loss Gen: -7.931005256978096e-07, Loss Blackbox: 1.6689259609847795e-06\n",
      "Step: 500, Epoch: 4, Loss Critic: -2.0463630789890885e-12, Loss Gen: -9.123076551986742e-07, Loss Blackbox: 1.430509428246296e-06\n",
      "Step: 600, Epoch: 4, Loss Critic: -2.2737367544323206e-13, Loss Gen: -8.014814056878095e-07, Loss Blackbox: 1.6521630641364027e-06\n",
      "Step: 700, Epoch: 4, Loss Critic: -3.410605131648481e-13, Loss Gen: -8.739169743421371e-07, Loss Blackbox: 1.5161906503635691e-06\n"
     ]
    }
   ],
   "source": [
    "ids_loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    tf.convert_to_tensor(dataset.values))\n",
    "train_data = train_data.shuffle(buffer_size=1024).batch(n_batch)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  for step, X_real in enumerate(train_data):\n",
    "    for _ in range(n_critics):\n",
    "      critic_loss = train_critic(X_real)\n",
    "    blackbox_loss, gen_loss = train_generator(len(X_real))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print('Step: {}, Epoch: {}, Loss Critic: {}, Loss Gen: {}, Loss Blackbox: {}'.format(\n",
    "          step, epoch, critic_loss, gen_loss, blackbox_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_accuracy(num_sample, num_sample_detected):\n",
    "  return num_sample_detected / num_sample\n",
    "\n",
    "def attack_success_rate(detection_rate_org, detection_rate_adv):\n",
    "  return detection_rate_org - detection_rate_adv\n",
    "\n",
    "def evade_increase_rate(detection_rate_org, detection_rate_adv):\n",
    "  return 1 - detection_rate_adv / detection_rate_org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sample = len(dataset)\n",
    "\n",
    "org_pred = blackbox.predict(dataset)\n",
    "org_pred = org_pred.round().argmax(axis=1)\n",
    "num_sample_detected_org = 0\n",
    "for s in org_pred:\n",
    "  if s == 1:\n",
    "    num_sample_detected_org += 1\n",
    "\n",
    "noise = tf.random.normal([num_sample, latent_dim])\n",
    "adv_sample = gen(noise, training=False)\n",
    "adv_pred = blackbox.predict(adv_sample)\n",
    "adv_pred = adv_pred.round().argmax(axis=1)\n",
    "num_sample_detected_adv = 0\n",
    "for s in adv_pred:\n",
    "  if s == 1:\n",
    "    num_sample_detected_adv += 1\n",
    "\n",
    "\n",
    "detection_rate_org = detection_accuracy(num_sample, num_sample_detected_org)\n",
    "detection_rate_adv = detection_accuracy(num_sample, num_sample_detected_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detection_rate_adv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
